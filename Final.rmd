---
title: "PSTAT231-final (231 Level)"
author: "Xianjun Yang and Selin Karabulut"
date: "`r format(Sys.Date(), '%B %d, %Y')`"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
library(tidyverse)
library(ROCR)
library(tree)
library(maptree)
library(class)
library(lattice)
library(ggridges)
library(superheat)
library(knitr)
library(plyr) 
library(rpart) 
library(dplyr) 
library(dendextend)
library(cluster)
library(ggplot2)
library(randomForest)
library(gbm)
library(e1071)
library(imager)

# set global chunk options: images will be 5x5 inches
knitr::opts_chunk$set(echo=TRUE, 
                      cache=TRUE, 
                      fig.width=5, 
                      fig.height=5,
                      fig.align='center')
indent1 = '    '      
indent2 = paste(rep(indent1, 2), collapse='')
```
$\textbf{Background}$
The presidential election in 2012 did not come as a surprise. Some predicted the outcome of the election correctly including Nate Silver, and many speculated his approach.

Despite the success in 2012, the 2016 presidential election came as a big surprise to many, and it was a clear example that even the current state-of-the-art technology can surprise us.

Answer the following questions in one paragraph for each.

1. What makes voter behavior prediction (and thus election forecasting) a hard problem?

    Data collection: there are always wrong data in data collection process; also the collection can not cover the majority of people from all counties; and the data collection process might ignore some population and focus too much on the others;
    Data integrity: there are too many factors that could influence the election results, and it is difficult to know all the influencing factors;
    Noise: there are too much noise for forecasting that we include in our data;
    Randomness of voters: some voter just make votes ramdomly on election day, which is independent of factors.

2. What was unique to Nate Silver's approach in 2012 that allowed him to achieve good predictions?

    Compared with Political pundits who are paid to spread opinions, Silver scientifically collect data and build the model all on the mathitical base rather than personal preference.
    The size of the database of Silver is very large, not only including the president election data, but also many other votes data; so he already simulated the prediction modle on different voting events and thus gained valuable experience; Silver learned from these experience, and adjust his model timely according to the latest changes.

3. What went wrong in 2016? What do you think should be done to make future predictions better?

    In 2016, prediction was wrong for many reasons, for example:
    Data bias: the data was not evenly collected from all counties all over US, and in the future data collection should be made more evenly so that no certain population will be ignored;
    failure for the polls: it has been verified that the polls are wrong because of some systemic bias on polls. In the furure, more accuate polls should be designed for prediction
    Ignorance of cettain factors: for example, the voting behavior of the minority decreases and was not considered;
    Voters behavior: some people believe Clinton was going to win and thus did not vote, while more Trump supporters are encouraged to vote.

$\textbf{Data}$
```{r data, message = FALSE}
## set the working directory as the file location
setwd(getwd())
## put the data folder and this handout file together.
## read data and convert candidate from string to factor
election.raw <- read_delim("data/election/election.csv", delim = ",") %>% mutate(candidate=as.factor(candidate))

census_meta <- read_delim("data/census/metadata.csv", delim = ";", col_names = FALSE) 
census <- read_delim("data/census/census.csv", delim = ",") 
```
$\textbf{Election data}$
The meaning of each column in election.raw is clear except fips. The accronym is short for Federal Information Processing Standard.

In our dataset, fips values denote the area (US, state, or county) that each row of data represent. For example, fips value of 6037 denotes Los Angeles County.
```{r Election_data, indent = indent1}
library(kableExtra)
kable(election.raw %>% filter(county == "Los Angeles County"))  %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), full_width=FALSE)
```
Some rows in election.raw are summary rows and these rows have county value of NA. There are two kinds of summary rows:

Federal-level summary rows have fips value of US.
State-level summary rows have names of each states as fips value.
$\textbf{4}$
Report the dimension of election.raw after removing rows with fips=2000. Provide a reason for excluding them. Please make sure to use the same name election.raw before and after removing those observations.

```{r remove data, indent = indent1}
kable(election.raw %>% filter(fips == 2000))  %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), full_width=FALSE)
```

```{r data dimension, indent = indent1}
election.raw <- election.raw%>%filter(fips!="2000")
dim(election.raw)
```
    
    Reason: there are no corresponding county records when flips=2000, so these rows are considered missing data and has to be removed.

$\textbf{Census data}$
Following is the first few rows of the census data:
```{r Census, indent = indent1}
head(census, n=6)
```
$\textbf{Census data: column metadata}$
Column information is given in metadata.
$\textbf{Data wrangling}$
5. Remove summary rows from election.raw data: i.e.,
* Federal-level summary into a `election_federal`.

* State-level summary into a `election_state`.

* Only county-level data is to be in `election`.
```{r data reduction, indent = indent1}
election <- election.raw %>% filter(fips!="US")
temp = is.na(as.numeric(election$fips))
election_state<- election[temp, ]
election<- election[!temp, ]
election_federal <- election.raw %>% filter(fips =="US")
```

6. How many named presidential candidates were there in the 2016 election? Draw a bar chart of all votes received by each candidate. You can split this into multiple plots or may prefer to plot the results on a log scale. Either way, the results should be clear and legible!

```{r candidates, indent = indent1}
dim(election_federal)[1]
```
      
      There are 32 presidential candidates in the 2016 election

```{r candidates1, indent = indent1}
data_candidates = data.frame( candidate = election_federal[1:8, 3],  votes = log(election_federal[1:8, 5]))
ggplot(data = data_candidates, aes(x= candidate, y = votes))+
  geom_bar(stat='identity')+
  labs("candidates~votes", y = "log(votes)", x = "candidate")
```
```{r candidates2, indent = indent1}
data_candidates = data.frame( candidate = election_federal[9:16, 3],  votes = log(election_federal[9:16, 5]))
ggplot(data = data_candidates, aes(x= candidate, y = votes))+
  geom_bar(stat='identity')+
  labs("candidates~votes", y = "log(votes)", x = "candidate")
```

```{r candidates3, indent = indent1}
data_candidates = data.frame( candidate = election_federal[17:24, 3],  votes = log(election_federal[17:24, 5]))
ggplot(data = data_candidates, aes(x= candidate, y = votes))+
  geom_bar(stat='identity')+
  labs("candidates~votes", y = "log(votes)", x = "candidate")
```

```{r candidates4, indent = indent1}
data_candidates = data.frame( candidate = election_federal[25:32, 3],  votes = log(election_federal[25:32, 5]))
ggplot(data = data_candidates, aes(x= candidate, y = votes))+
  geom_bar(stat='identity')+
  labs("candidates~votes", y = "log(votes)", x = "candidate")
```
7. Create variables county_winner and state_winner by taking the candidate with the highest proportion of votes. Hint: to create county_winner, start with election, group by fips, compute total votes, and pct = votes/total. Then choose the highest row using top_n (variable state_winner is similar).
```{r 7, indent = indent1}
county_winner <- election %>%
  group_by(fips) %>%
  mutate(total=sum(votes), pct=votes/total) %>%
  top_n(1, pct)

state_winner <- election_state %>%
  group_by(fips) %>%
  mutate(total=sum(votes), pct=votes/total) %>%
  top_n(1, pct)
```

$\textbf{Visualization}$
Visualization is crucial for gaining insight and intuition during data mining. We will map our data onto maps.

The R package ggplot2 can be used to draw maps. Consider the following code.
```{r Visualization, indent = indent1}
states <- map_data("state")
ggplot(data = states) + 
  geom_polygon(aes(x = long, y = lat, fill = region, group = group), color = "white") + 
  coord_fixed(1.3) +
  guides(fill=FALSE)  # color legend is unnecessary and takes too long
```
8. Draw county-level map by creating counties = map_data("county"). Color by county
```{r county Visualization, indent = indent1}
counties <- map_data("county")
ggplot(data = counties) + 
  geom_polygon(aes(x = long, y = lat, fill = subregion, group = group), color = "white") + 
  coord_fixed(1.3) +
  guides(fill=FALSE)  # color legend is unnecessary and takes too long
```
9. Now color the map by the winning candidate for each state. First, combine states variable and state_winner we created earlier using left_join(). Note that left_join() needs to match up values of states to join the tables. A call to left_join() takes all the values from the first table and looks for matches in the second table. If it finds a match, it adds the data from the second table; if not, it adds missing values:
Here, we'll be combing the two datasets based on state name. However, the state names are in different formats in the two tables: e.g. AZ vs. arizona. Before using left_join(), create a common column by creating a new column for states named fips = state.abb[match(some_column, some_function(state.name))]. Replace some_column and some_function to complete creation of this new column. Then left_join(). Your figure will look similar to state_level New York Times map.
```{r add column, indent = indent1}
fips = state.abb[match(states$region, tolower(state.name))]
states$fips = fips
```
```{r statemap, indent = indent1}
states_combined = left_join(states, state_winner, by = "fips")
ggplot(data = states_combined) + 
  geom_polygon(aes(x = long, y = lat, fill = candidate, group = group), color = "white") + 
  coord_fixed(1.3) +
  guides(fill=FALSE)  # color legend is unnecessary and takes too long
```
10. The variable county does not have fips column. So we will create one by pooling information from maps::county.fips. Split the polyname column to region and subregion. Use left_join() combine  county.fips into county. Also, left_join() previously created variable county_winner. Your figure will look similar to county-level New York Times map.
```{r countyadd, indent = indent1}
countyinfo = maps::county.fips
regiontotal = unlist( strsplit(countyinfo$polyname, ',') )
index = seq(1,6170,2)
region = regiontotal[index]
subregion = regiontotal[-index]

countyinfo$region = region
countyinfo$subregion = subregion
county_combined = left_join(counties, countyinfo)
county_combined$fips = as.character(county_combined$fips)
county_final = left_join(county_winner, county_combined, by = "fips")
```
```{r county candidate map, indent = indent1}
ggplot(data = county_final) + 
  geom_polygon(aes(x = long, y = lat, fill = candidate, group = group), color = "white") + 
  coord_fixed(1.3) +
  guides(fill=FALSE)  # color legend is unnecessary and takes too long
```
11. Create a visualization of your choice using census data. Many exit polls noted that demographics played a big role in the election. Use this Washington Post article and this R graph gallery for ideas and inspiration.


12. The census data contains high resolution information (more fine-grained than county-level). In this problem, we aggregate the information into county-level data by computing TotalPop-weighted average of each attributes for each county. Create the following variables:

Clean census data census.del: start with census, filter out any rows with missing values, convert {Men, Employed, Citizen} attributes to percentages (meta data seems to be inaccurate), compute Minority attribute by combining {Hispanic, Black, Native, Asian, Pacific}, remove these variables after creating Minority, remove {Walk, PublicWork, Construction}.
Many columns seem to be related, and, if a set that adds up to 100%, one column will be deleted.
```{r 12, indent = indent1}
census.del <- na.omit(census)
#convert {`Men`, `Employed`, `Citizen`} attributes to a percentages
census.del$Men <- census.del$Men/census.del$TotalPop
census.del$Employed <- census.del$Employed/census.del$TotalPop
census.del$Citizen <- census.del$Citizen/census.del$TotalPop
#combining {Hispanic, Black, Native, Asian, Pacific}
census.del$Minority <- census.del$Hispanic+census.del$Black+census.del$Native+census.del$Asian+census.del$Pacific
#remove {`Walk`, `PublicWork`, `Construction`}
census.del <- select(census.del, -Walk,-PublicWork,-Construction)
#Remove colunms that are unneccessary
census.del <- select(census.del, -Women)
census.del<- select(census.del,-Hispanic,-Black,-Native,-Asian,-Pacific)
census.del
```


Sub-county census data, census.subct: start with census.del from above, group_by() two attributes {State, County}, use add_tally() to compute CountyTotal. Also, compute the weight by TotalPop/CountyTotal.
```{r 12-2, indent = indent1}
#group by {`State`, `County`}
census.subct <- census.del %>%
group_by(State,County)
#compute `CountyTotal`
census.subct <- add_tally(census.subct,TotalPop,sort=FALSE)
colnames(census.subct)[29] <- "CountyTotal"
#compute the weight by `TotalPop/CountyTotal`
census.subct$Weight <- census.subct$TotalPop/census.subct$CountyTotal
census.subct
```
County census data, census.ct: start with census.subct, use summarize_at() to compute weighted sum
Print few rows of census.ct:
```{r 12-3, indent = indent1}
census.ct<-census.subct %>%
summarise_at(vars(Men:CountyTotal), funs(weighted.mean(., Weight)))
census.ct <- data.frame(census.ct)
head(census.ct)
```
13. Run PCA for both county & sub-county level data. Save the first two principle components PC1 and PC2 into a two-column data frame, call it ct.pc and subct.pc, respectively. Discuss whether you chose to center and scale the features before running PCA and the reasons for your choice. What are the three features with the largest absolute values of the first principal component? Which features have opposite signs and what does that mean about the correaltion between these features?

creating pca objects
```{r}
ct.pca <- prcomp(census.ct[3:28], center = TRUE, scale=TRUE)
subct.pca <- prcomp(census.subct[4:30], center = TRUE, scale=TRUE)
```

getting the principal components
```{r}
ct.pc <- data.frame(ct.pca$rotation[,1:2])
subct.pc <- data.frame(subct.pca$rotation[,1:2])
```


```{r}
top_n(abs(ct.pc[1]), 3)
top_n(abs(subct.pc[1]), 3)
```


    Scele and center is used, because the columns values have a large difference and also have different meanings;
    The top 3 prominent loadings at the county level of PC1 are IncomePerCap, Poverty and ChildPoverty for ct.pc; IncomePerCap is negative while the other two are positive, which means IncomePerCap has opposite effect compared with the other two
    The top 3 prominent loadings at the county level of PC1 are IncomePerCap, Poverty and Professional for subct.pc;
14. Determine the number of minimum number of PCs needed to capture 90% of the variance for both the county and sub-county analyses. Plot proportion of variance explained (PVE) and cumulative PVE for both county and sub-county analyses.
```{r}
pr.var = ct.pca$sdev ^2
pve.ct = pr.var/sum(pr.var)
cumulative_pve.ct <- cumsum(pve.ct)
```

```{r, eval=FALSE}
# This will put the next two plots side by side  
par(mfrow=c(1, 2))
# Plot proportion of variance explained
plot(pve.ct, type="l", lwd=3, xlab="Principal Component",
     ylab="PVE of county", ylim =c(0,1))
plot(cumulative_pve.ct, type="l", lwd=3, xlab="Principal Component ",
     ylab=" Cumulative PVE of county ", ylim=c(0,1))
```

```{r, eval=FALSE}
round(pve.ct[1:25], 2)
sum(pve.ct[1:13])
sum(pve.ct[1:14])
```
  
    So the minimum number of PCs needed to capture 90% of the variance for the county is 14.
```{r}
pr.varsubct = subct.pca$sdev ^2
pve.subct = pr.varsubct/sum(pr.varsubct)
cumulative_pve.subct <- cumsum(pve.subct)
```

```{r, eval=FALSE}
# This will put the next two plots side by side  
par(mfrow=c(1, 2))
# Plot proportion of variance explained
plot(pve.subct, type="l", lwd=3, xlab="Principal Component",
     ylab="PVE of subcounty", ylim =c(0,1))
plot(cumulative_pve.subct, type="l", lwd=3, xlab="Principal Component ",
     ylab=" Cumulative PVE of subcounty ", ylim=c(0,1))
``` 
```{r, eval=FALSE}
round(pve.subct[1:25], 2)
sum(pve.subct[1:15])
sum(pve.subct[1:16])
```
  
    So the minimum number of PCs needed to capture 90% of the variance for the county is 16.
## Clustering
15. With census.ct, perform hierarchical clustering with complete linkage. Cut the tree to partition the observations into 10 clusters. Re-run the hierarchical clustering algorithm using the first 5 principal components of ct.pc as inputs instead of the originald features. Compare and contrast the results. For both approaches investigate the cluster that contains San Mateo County. Which approach seemed to put San Mateo County in a more appropriate clusters? Comment on what you observe and discuss possible explanations for these observations.

```{r}
Scensus.ct <- scale(census.ct[,-c(1,2)],center =T,scale = T)
dist_census.ct <- dist(Scensus.ct)
hc.census.ct <- hclust(dist_census.ct)
hc.census.ct <- cutree(hc.census.ct, k = 10)
table(hc.census.ct)
```
```{r}
hc.census.pc <- cutree(hclust(dist(scale(data.frame(ct.pca$x[,1:5])))),k=10)
table(hc.census.pc)
```

```{r}
census.ct[227,]
```


```{r}
hc.census.ct[227]
```

```{r}
hc.census.pc[227]
```

```{r}
hc.census.ct.df<-as.data.frame(hc.census.ct) ##change into data frame
hc.census.pc.df<-as.data.frame(hc.census.pc)
sanmateo.ct<- data.frame(hc.census.ct.df,census.ct)## combine into 1 file
sanmateo.ct <- sanmateo.ct %>%
group_by(hc.census.ct) ## group the census.ct data according to cluster id
head(sanmateo.ct)
```

```{r}
sanmateo.pc<- data.frame(hc.census.pc.df,census.ct)## combine into 1 file
sanmateo.pc <- sanmateo.pc %>%
group_by(hc.census.pc) ## group the census.ct data according to cluster id
head(sanmateo.pc)
```

    In census.ct, San Mateo has index at row no.227 so we look for no.227 in the cluster lists of census.ct and ct.pc San Mateo is in the second cluster of hc.census.ct and the first cluster of hc.census.pc
    
    When using census.ct, the county San Mateo is placed into cluster 2. But when using the first five principal components, San Mateo is placed into cluster 1. Furthermore, when looking at the cluster assignments attached to the original data (in the dataframes dataclustersa and dataclustersb) we see that when San Mateo is placed in cluster 2, it appears to be more in line with cluster guidelines (we want the elements in the clusters to be as similar as possible); there are less Alabama counties inside cluster 2 with San Mateo for example (which we would expect since San Mateo is a county from California). But when San Mateo is placed into cluster 1 there are way more differing counties in its cluster (most of Alabama counties are in this cluster for example). This is most likely due to the fact that the first five principal components do not describe most of the variance in census.ct, thus there are disagreements in the clustering.
    
    When using clustering, we want the clusters that have been found to represent true subgroups in the data. In this case, we want true subgroups of counties. For counties to be in the same cluster, their attributes should be similar to each other like similar level of income or similar proportion of minority. A good cluster therefore would contain counties that are similar to each other in all of the attributes, or the distance between these counties point in a 28-dimensional space should be very close to each other. There are other issues to consider about clustering like clustering the noise or clustering outliers that actually do not belong to any clusters present.
    
    To look at the cluster as a whole to see the association with other county, i.e, which counties are grouped together, we can change hc.census.ct and hc.census.pc into a data frames, merge it with census.ct and then group them by the cluster index. From these table, we can see that the clustering using principal components is not as good as the clustering using census.ct since other cities within the cluster of San Mateo does not seem to be similar to it in terms of characteristic. This could be because the principal components does not explain all of the data and thus could be omitting important attributes that could be used to cluster the counties.
    
## Classification
In order to train classification models, we need to combine county_winner and census.ct data. This seemingly straightforward task is harder than it sounds. Following code makes necessary changes to merge them into election.cl for classification.
```{r}
tmpwinner <- county_winner %>% ungroup %>%
  mutate(state = state.name[match(state, state.abb)]) %>%               ## state abbreviations
  mutate_at(vars(state, county), tolower) %>%                           ## to all lowercase
  mutate(county = gsub(" county| columbia| city| parish", "", county))  ## remove suffixes
tmpcensus <- census.ct %>% mutate_at(vars(State, County), tolower)

election.cl <- tmpwinner %>%
  left_join(tmpcensus, by = c("state"="State", "county"="County")) %>% 
  na.omit

## save meta information
election.meta <- election.cl %>% select(c(county, fips, state, votes, pct, total))

## save predictors and class labels
election.cl = election.cl %>% select(-c(county, fips, state, votes, pct, total))
```
Using the following code, partition data into 80% training and 20% testing:
```{r}
set.seed(10) 
n <- nrow(election.cl)
in.trn <- sample.int(n, 0.8*n) 
trn.cl <- election.cl[ in.trn,]
tst.cl <- election.cl[-in.trn,]
```


```{r}
set.seed(20) 
nfold <- 10
folds <- sample(cut(1:nrow(trn.cl), breaks=nfold, labels=FALSE))
```


```{r}
calc_error_rate = function(predicted.value, true.value){
  return(mean(true.value!=predicted.value))
}
records = matrix(NA, nrow=3, ncol=2)
colnames(records) = c("train.error","test.error")
rownames(records) = c("tree","logistic","lasso")
```
## Classification
16. Decision tree: train a decision tree by cv.tree(). Prune tree to minimize misclassification error. Be sure to use the folds from above for cross-validation. Visualize the trees before and after pruning. Save training and test errors to records variable. Intepret and discuss the results of the decision tree analysis. Use this plot to tell a story about voting behavior in the US (remember the NYT infographic?)
```{r}
cv.tree <- tree(candidate~., data = trn.cl)
summary(cv.tree)
```


```{r}
draw.tree(cv.tree, cex=.5, nodeinfo=TRUE)
title("Un-Pruned Tree")
```


Prune
```{r}
cv <- cv.tree(cv.tree, folds, method='misclass' )
# Best size
best.size.cv = min(cv$size[which(cv$dev == min(cv$dev))])
best.size.cv
```

```{r}
# Plot size vs. cross-validation error rate
plot(cv$size , cv$dev,
xlab = "Number of leaves", ylab = "CV Misclassification Error", col = "red", main="CV")
abline(v=best.size.cv, lty=2)
```

```{r}
pruned <- prune.tree(cv.tree, best=best.size.cv)
draw.tree(pruned, cex=.5, nodeinfo=TRUE)
title("Pruned Tree")
```


```{r}
pred.test_tree = predict(pruned, tst.cl, type="class")
calc_error_rate(pred.test_tree, tst.cl$candidate)
```


```{r}
# Predict on training set
pred.train_tree = predict(pruned, trn.cl, type="class")
calc_error_rate(pred.train_tree, trn.cl$candidate)
```
```{r}
records[1,] = c(calc_error_rate(pred.train_tree,trn.cl$candidate),calc_error_rate(pred.test_tree, tst.cl$candidate))
records
```

    From the result, we can see that: 
    Donald Trump won the vast maiority of commuting on public transportation or counties with more white people, while Hillary Clinton won the maiority of counties with higher unemployment rate or not relaying on commuting on public transportation.
17. Run a logistic regression to predict the winning candidate in each county. Save training and test errors to records variable. What are the significant variables? Are the consistent with what you saw in decision tree analysis? Interpret the meaning of a couple of the significant coefficients in terms of a unit change in the variables.

```{r}
logistic_log <- glm(candidate~., data = trn.cl, family = binomial)
pre.log_train = ifelse(predict(logistic_log, type = "response") > 0.5, "Hillary Clinton", "Donald Trump")
pre.log_test = ifelse(predict(logistic_log, tst.cl, type = "response") > 0.5, "Hillary Clinton", "Donald Trump")
coef_unpenalized = data.frame(logistic_log$coefficients)
coef_unpenalized
top_n(abs(coef_unpenalized), 5)
```
    
    From the top 4 significant coefficients: Men, Citizen, Employed, FamilyWork plays a significant role in the voting bevior; Except for Employed is consistent with the decision tree, the others are not consistent.
    Interpret the meaning of a couple of the significant coefficients in terms of a unit change in the variables: the odds will be multiplied by $e_{coefficient}$ for any a unit change, for eample: $e_{Men}$ or $e_{Citizen}$
    
```{r}
log_train_error = calc_error_rate(pre.log_train, trn.cl$candidate)
log_test_error = calc_error_rate(pre.log_test, tst.cl$candidate)
records[2,] = c(log_train_error, log_test_error)
records
```



18. You may notice that you get a warning glm.fit: fitted probabilities numerically 0 or 1 occurred. As we discussed in class, this is an indication that we have perfect separation (some linear combination of variables perfectly predicts the winner). This is usually a sign that we are overfitting. One way to control overfitting in logistic regression is through regularization. Use the cv.glmnet function from the glmnet library to run K-fold cross validation and select the best regularization parameter for the logistic regression with LASSO penalty. Reminder: set $\alpha$=1 to run LASSO regression, set $\lambda$ = c(1, 5, 10, 50) * 1e-4 in cv.glmnet() function to set pre-defined candidate values for the tuning parameter $\lambda$. This is because the default candidate values of $\lambda$ in cv.glmnet() is relatively too large for our dataset thus we use pre-defined candidate values. What is the optimal value of $\lambda$ in cross validation? What are the non-zero coefficients in the LASSO regression for the optimal value of $\lambda$ ? How do they compare to the unpenalized logistic regression? Save training and test errors to the records variable.
```{r}
library(glmnet)
lasso_log = cv.glmnet(x = data.matrix(select(trn.cl, -candidate)), y = data.matrix( select(trn.cl, candidate) ),  alpha=1, nfolds = 10, lambda = c(1, 5, 10, 50) * 1e-4, family = 'binomial')
lasso_log$lambda.min
```
  
    So the optimal value of $\lambda$ in cross validation is 5e-04
```{r}
coef_lasso = coef(lasso_log, lasso_log$lambda.min)
coef_lasso
```
    
    Above is the non-zero coefficients in the LASSO regression for the optimal value except for SelfEmployed and Minority which have zero coefficients. Compared with the the unpenalized logistic regression, LASSO result has zero-coefficients while unpenalized results are all non-zero.
    
```{r}
pre.lasso_train = ifelse(predict(lasso_log, s = "lambda.min", newx = data.matrix(select(trn.cl, -candidate)), type = "response") > 0.5, "Hillary Clinton", "Donald Trump")
pre.lasso_test = ifelse(predict(lasso_log, s = "lambda.min", newx = data.matrix(select(tst.cl, -candidate)), type = "response") > 0.5, "Hillary Clinton", "Donald Trump")

log_train_error = calc_error_rate(pre.lasso_train, trn.cl$candidate)
log_test_error = calc_error_rate(pre.lasso_test, tst.cl$candidate)
records[3,] = c(log_train_error, log_test_error)
records
```

19. Compute ROC curves for the decision tree, logistic regression and LASSO logistic regression using predictions on the test data. Display them on the same plot. Based on your classification results, discuss the pros and cons of the various methods. Are the different classifiers more appropriate for answering different kinds of questions about the election?

Since decesion tree does not explictly output a probability for class labels, we use 200 bootstrap replicates of the testing data to predict the class label.
```{r}
set.seed(8)
test2 <- tst.cl
train2 <- trn.cl

for(i in 1:200){
  train_index <- sample(nrow(trn.cl), replace = TRUE)
  nfold <- 10
  folds <- sample(cut(1:nrow(trn.cl[train_index, ]), breaks=nfold, labels=FALSE))
  
  cv.tree <- tree(candidate~., data = trn.cl[train_index, ])
  cv <- cv.tree(cv.tree, folds, method='misclass' )
  pruned <- prune.tree(cv.tree, best=best.size.cv)
  precandidate = as.numeric(predict(pruned, tst.cl, type='class')) -1
  test2 <- cbind(test2, precandidate)
}

test2 <- rowSums(test2[, 28:227])
probs <- test2/200

reals = ifelse(tst.cl$candidate == "Hillary Clinton", 1, 0)

predCandidate <- prediction(probs, reals)

perfCandidate = performance(predCandidate, measure="tpr", x.measure="fpr")

plot(perfCandidate, col=2, lwd=3, main="ROC curve of Tree")
abline(0,1)
```


```{r}
test_label = ifelse(tst.cl$candidate == "Hillary Clinton", 1, 0)
pre.log_test = predict(logistic_log, type = "response", tst.cl) 
pred_log = prediction(pre.log_test, test_label)
perf_log = performance(pred_log, measure = "tpr", x.measure = "fpr")
log_data = data.frame(x = perf_log@x.values[[1]], y = perf_log@y.values[[1]], type = "logistic")

pre.lasso_test = predict(lasso_log, s = "lambda.min", newx = data.matrix(select(tst.cl, -candidate)), type = "response")
pred_lasso = prediction(pre.lasso_test, test_label)
perf_lasso = performance(pred_lasso, measure = "tpr", x.measure = "fpr")
lasso_data = data.frame(x = perf_lasso@x.values[[1]], y = perf_lasso@y.values[[1]], type = "Lasso")

tree_data = data.frame(x = perfCandidate@x.values[[1]], y = perfCandidate@y.values[[1]], type = "Tree")
data_combined = rbind(log_data, lasso_data, tree_data)

#plot ROC
ggplot(data= data_combined, aes(x=x, y=y, color=type)) +
  geom_line() +
  geom_abline(intercept = 0, slope = 1, color = "grey", size = 0.5)+
  labs(title = "ROC curves", x="FPR", y="TPR")

```

```{r}
tree_auc = performance(predCandidate, "auc")@y.values
logistic_auc = performance(pred_log, "auc")@y.values
lasso_auc = performance(pred_lasso, "auc")@y.values
data2 = data.frame(tree=tree_auc[[1]], logistic = logistic_auc[[1]], lasso=lasso_auc[[1]])
row.names(data2) = c("AUC")
data2
```
    
    As the significant cofficients are quite different for different methods, different classifiers are more appropriate for answering different kinds of questions about the election. 
    From the AUC results, lasso has higher AUC value so lasso should be perferred; 

## Taking it further
20. This is an open question. Interpret and discuss any overall insights gained in this analysis and possible explanations. Use any tools at your disposal to make your case: visualize errors on the map, discuss what does/doesn't seems reasonable based on your understanding of these methods, propose possible directions (collecting additional data, domain knowledge, etc). In addition, propose and tackle at least one more interesting question.



